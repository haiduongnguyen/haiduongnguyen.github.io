---
title: Linear Regression
---

üîô [Back to Home](/)

## Linear regression



# Definition

![image.png](images/1.png)

Linear regression 1 variable

y = w*x + b

Linear regression for multiple variable

y = w1*x1 + w2*x2 + ‚Ä¶ + wn*xn + b

Linear regression with polymonial variable (treat polynorminal as new variable by feature engineering)

y = w1*x1 + w2*x1^2 + ‚Ä¶ + b

1. Application

Use linear regression in :

- Predict continuous output: house price, stock, creidt score,‚Ä¶
- 
1. Solving

2 ways to solve linear regression:

- Gradient descent
- Matrix

Detail solution:

- Gradient descent

 Step 1: Init w & b

```python
# khoi tao w

w_init = -0.01
w_init

# khoi tao b

b_init = 46
b_init

learning_rate = 0.0000001
loss_history = []
w_history = []
b_history = []
```

Step 2: write mse calculation

```
def calculate_mse(y_pred, y):
    mse = 0
    for i in range(len(y)):
        mse += ((y_pred[i] - y[i])**2)/len(y)
    
    return mse
```

Step 3: write gradient descent calculation

```python
def grad_y_by_w(w, b, x, y):
    tot_grad = 0
    for ele in range(len(x)):
        tot_grad += (1/len(x))*(w*x[ele] + b - y[ele])*x[ele]
    return tot_grad
        
def grad_y_by_b(w, b, x, y):
    tot_grad = 0
    for ele in range(len(x)):
        tot_grad += (1/len(x))*(w*x[ele] + b - y[ele])
    return tot_grad

```

Step 6: Loop calculate w and b by gradient descent 

```python
for epoch in range(30):
    print("================================")
    print(f"Run for epach {epoch}")
    
    if epoch == 0:
        w = w_init
        b = b_init
        y_pred = w*x + b 
        
    if epoch >= 1:
        grad_l_by_w = grad_y_by_w(w, b, x, y)
        grad_l_by_b = grad_y_by_b(w, b, x, y)
        print(grad_l_by_w)
        print(grad_l_by_b)
        
        w = w - learning_rate*grad_l_by_w
        b = b - learning_rate*grad_l_by_b
        
        print("w: ", w)
        print("b: ", b)
        
        y_pred = w*x + b 
    
    
    loss = calculate_mse(y_pred, y)
    # print(loss)
    loss_history.append(loss)
    print("Loss: ", loss)
    w_history.append(w)
    b_history.append(b)
    
```

Step 7:

Compare to SGD function from sklearn

```python
import numpy as np
from sklearn.linear_model import LinearRegression, SGDRegressor

# data
X = np.array([[1], [2], [3], [4], [5]], dtype=float)
y = np.array([1.2, 1.9, 3.2, 3.9, 5.1])

# exact OLS
lr = LinearRegression().fit(X, y)
print("LinearRegression coef:", lr.coef_, "intercept:", lr.intercept_)

# gradient descent version
sgd = SGDRegressor(max_iter=10000, eta0=0.01, learning_rate='constant').fit(X, y)
print("SGDRegressor coef:", sgd.coef_, "intercept:", sgd.intercept_)

```

- Matrix

![image.png](images/2.png)

![image.png](images/3.png)

Code:

```python
x_temp = np.concatenate([np.ones((n, 1)), np.reshape(x, (n,1))], axis=1)

x_temp

w_temp = np.linalg.inv((x_temp.T) @ x_temp) @ (x_temp.T) @ y

w_temp

b = w_temp[0]
w = w_temp[1]
```

```python
ymean = y.mean()

xmean = x.mean()

ymean

yoffset = y - ymean 
xoffset = x - xmean 

yoffset

w = np.linalg.inv(((x_temp.T) @ x_temp))  @ (x_temp.T) @ y_temp

w

b = ymean - xmean*w 
b
```

Benchmark of Gradient descent vs Matrix

```python
import time
import numpy as np
from sklearn.linear_model import LinearRegression, SGDRegressor

def benchmark(n_samples, n_features):
    X = np.random.randn(n_samples, n_features)
    y = np.random.randn(n_samples)
    
    # LinearRegression
    start = time.time()
    try:
        LinearRegression().fit(X, y)
        lr_time = time.time() - start
    except Exception as e:
        lr_time = str(e)
    
    # SGDRegressor
    start = time.time()
    SGDRegressor(max_iter=1000).fit(X, y)
    sgd_time = time.time() - start
    
    return lr_time, sgd_time

sizes = [(1000, 1000), (2000, 2000), (5000, 5000), (10000, 1000), (1000, 10000)]
for n, d in sizes:
    print(f"n={n}, d={d}:", benchmark(n, d))

```

1. Linear regression assumption

| Gi·∫£ thuy·∫øt | C√°ch ki·ªÉm ƒë·ªãnh | C√¥ng c·ª• |
| --- | --- | --- |
| Tuy·∫øn t√≠nh | V·∫Ω bi·ªÉu ƒë·ªì residuals vs fitted values | sns.residplot() |
| ƒê·ªôc l·∫≠p ( durbin-watson √°p d·ª•ng v·ªõi time series) | Ki·ªÉm ƒë·ªãnh Durbin-Watson | durbin_watson() |
| Ph∆∞∆°ng sai kh√¥ng ƒë·ªïi | V·∫Ω residuals vs fitted, ki·ªÉm ƒë·ªãnh Breusch-Pagan | statsmodels |
| Ph√¢n ph·ªëi chu·∫©n | Ki·ªÉm ƒë·ªãnh Shapiro-Wilk ho·∫∑c v·∫Ω histogram + Q-Q plot | shapiro(), qqplot() |
| Kh√¥ng ƒëa c·ªông tuy·∫øn | T√≠nh VIF (Variance Inflation Factor) | variance_inflation_factor() |

Ph·∫ßn d∆∞ residual (resid) c·∫ßn tu√¢n theo ph√¢n ph√¥tis chu·∫©n, c√≥ ph∆∞∆°ng sai kh√¥ng ƒë·ªïi 

ƒê·ªÉ ki·ªÉm ch·ª©ng th∆∞·ªùng s·∫Ω ch·∫°y Shapiro-wilk ƒë·ªÉ t√≠nh ra p-value 

Shapiro-wilk: H0: resid tu√¢n theo ph√¢n ph·ªëi chu·∫©n

p-value < 0.05 ‚Üí B√°c b·ªè H0

Tuy nhi√™n n·∫øu m·∫´u l·ªõn ‚Üí Shapiro th∆∞·ªùng cho ra p-value < 0.05 n√™n s·∫Ω c·∫ßn th√™m v·∫Ω bi·ªÉu ƒë·ªì tr·ª±c quan ra

Bi·ªÉu ƒë·ªì Q-Q

```python
import statsmodels.api as sm
import matplotlib.pyplot as plt

sm.qqplot(np.array(resid), line='45', fit=True)
plt.show()

```

Bi·ªÉu ƒë·ªì histogram

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(resid, kde=True)
plt.show()

```

Ki·ªÉm ƒë·ªãnh ƒëa c·ªông tuy·∫øn:

S·ª≠ d·ª•ng h√†m variance_inflation_factor: https://en.wikipedia.org/wiki/Variance_inflation_factor

B·∫£n ch·∫•t l√† predict 1 feature Xi d·ª±a v√†o c√°c X c√≤n l·∫°i, ƒëo r square c·ªßa m√¥ h√¨nh 

N·∫øu VIF > 5 (t√≠nh ra th√¨ l√† R2 > 0.8) th√¨ bi·∫øn i c·∫ßn xem x√©t l√† b·ªã ƒëa c·ªông tuy·∫øn 

```python
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Gi·∫£ s·ª≠ X l√† DataFrame ch·ª©a c√°c bi·∫øn ƒë·ªôc l·∫≠p
X = add_constant(x)   # th√™m c·ªôt h·∫±ng s·ªë (intercept)

vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)
```

![image.png](images/4.png)

1. X·ª≠ l√≠ khi b·ªã c√°c v·∫•n ƒë·ªÅ 

Khi ƒë√£ b·ªã ƒëa c·ªông tuy·∫øn, th√¨ c√≥ nh·ªØng c√°ch x·ª≠ l√≠ sau:

- Lo·∫°i b·ªõt bi·∫øn gi·∫£i th√≠ch (predictors) b·ªã tr√πng l·∫∑p th√¥ng tin.
- K·∫øt h·ª£p c√°c bi·∫øn c√≥ t∆∞∆°ng quan cao (v√≠ d·ª• t·∫°o ch·ªâ s·ªë t·ªïng h·ª£p).
- S·ª≠ d·ª•ng **Regularization methods** (Ridge, Lasso).

C√°ch 1: B·ªè b·ªõt bi·∫øn ƒëi 

C√°ch 2: K·∫øt h·ª£p c√°c bi·∫øn = ph∆∞∆°ng ph√°p PCA 

## 1Ô∏è‚É£ Eigenvalue l√† g√¨?

Cho 1 ma tr·∫≠n vu√¥ng AAA, ta gi·∫£i ph∆∞∆°ng tr√¨nh:

Av = Œªv

A: n x n 

v: n x 1 

Œª: 1 so float 

Av=Œªv

- vvv = **eigenvector** (vector ri√™ng).
- Œª\lambdaŒª = **eigenvalue** (tr·ªã ri√™ng).

√ù nghƒ©a: khi nh√¢n ma tr·∫≠n A v·ªõi vector v, ta ch·ªâ thay ƒë·ªïi ƒë·ªô d√†i (scale) vector ƒë√≥ l√™n Œª\lambdaŒª l·∫ßn, **m√† kh√¥ng ƒë·ªïi h∆∞·ªõng**.

V√≠ d·ª•: t∆∞·ªüng t∆∞·ª£ng vvv l√† m·ªôt m≈©i t√™n, sau ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh AAA, m≈©i t√™n v·∫´n ch·ªâ c√πng h∆∞·ªõng, ch·ªâ d√†i ra ho·∫∑c ng·∫Øn l·∫°i.

## 2Ô∏è‚É£ Eigenvalues trong PCA c√≥ √Ω nghƒ©a g√¨?

Trong PCA:

- A ch√≠nh l√† **ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai** C.
- Eigenvalue c·ªßa m·ªói eigenvector cho bi·∫øt **l∆∞·ª£ng ph∆∞∆°ng sai d·ªØ li·ªáu gi·ªØ l·∫°i** tr√™n tr·ª•c ƒë√≥.

‚û°Ô∏è **Eigenvalue c√†ng l·ªõn ‚Üí tr·ª•c (eigenvector) ƒë√≥ ch·ª©a nhi·ªÅu th√¥ng tin h∆°n.**

## 3Ô∏è‚É£ S·ª≠ d·ª•ng eigenvalues ƒë·ªÉ l√†m g√¨ trong PCA?

- **X·∫øp h·∫°ng c√°c tr·ª•c PCA:** ta s·∫Øp x·∫øp eigenvalues t·ª´ l·ªõn ƒë·∫øn nh·ªè ƒë·ªÉ bi·∫øt tr·ª•c n√†o quan tr·ªçng nh·∫•t.
- **Ch·ªçn s·ªë chi·ªÅu kkk:** th∆∞·ªùng ch·ªçn kkk sao cho t·ªïng eigenvalues chi·∫øm ƒë·ªß % ph∆∞∆°ng sai (VD: 95%).

C√¥ng th·ª©c t√≠nh **variance explained ratio**:

Explained¬†Variance¬†Ratioi

![image.png](images/5.png)

Khi ƒë√≥ t ch·ªçn top k trong m PCA 

‚Üí Gi·ªØ l·∫°i ƒë∆∞·ª£c % l·ªõn th√¥ng tin trong ma tr·∫≠n 

T√¨m Œª v√† v ki·ªÉu g√¨: ‚Üí Khi A l√† n x n th√¨ s·∫Ω c√≥ n c·∫∑p (Œª, v) 

![image.png](images/6.png)

Sau ƒë√≥ l·∫≠p ma tr·∫≠n g·ªìm c√°c [v] ƒë√¢y l√† ma tr·∫≠n m·ªõi v·∫´n gi·ªØ ƒë·ªß th√¥ng tin c·ªßa A 

Ch·ªçn top c√°c h√†ng m√† v·∫´n gi·ªØ ƒë∆∞·ª£c nhi·ªÅu th√¥ng tin nh·∫•t 

‚Üí Gi·∫£m ƒë∆∞·ª£c s·ªë chi·ªÅu d·ªØ li·ªáu t·ª´ m ‚Üí k 

C√°ch 3: S·ª≠ d·ª•ng Regulation Lasso v√† Ridge

![image.png](images/7.png)

![image.png](images/8.png)

C√°ch gi·∫£i:

![image.png](images/9.png)

![image.png](images/10.png)

C√°ch gi·∫£i: 

![image.png](images/11.png)