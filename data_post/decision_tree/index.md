
---
title: Decision Tree
---

üîô [Back to Home](/)

## Decision tree: 

Reference link: https://machinelearningcoban.com/tabml_book/ch_model/decision_tree.html 


## Definition

![image.png](images/1.png)

T·∫°i m·ªói b∆∞·ªõc:

- Duy·ªát qua t·ª´ng feature
- Duy·ªát qua t·ª´ng ng∆∞·ª°ng c·ªßa feature
- T√≠nh gini c·ªßa t·ª´ng ng∆∞·ª°ng
- L·∫•y gini b√© nh·∫•t ƒë·ªÉ chia c√¢y th√†nh 2 c√¢y nh·ªè left v√† right

T·∫°i sao l·∫°i l·∫•y gini b√©? 

V√≠ d·ª• khi c·∫Øt 1 ng∆∞·ª°ng m√† c·∫£ 2 c√¢y Left v√† Right b·ªã l·∫´n 2 t·∫≠p v√†o nhau (t·ª©c l√† k t·ªët) 

‚Üí c√¢y left c√≥: gini = 1 - 0.5^2 - 0.5^2 = 0.5

‚Üí c√¢y right c√≥: gini = 1 - 0.5^2 - 0.5^2 = 0.5

C√≤n v√≠ d·ª• khi c·∫Øt 1 ng∆∞·ª°ng t·ªët, t·ª©c l√† c√≥ kh·∫£ nƒÉng ph√¢n bi·ªát, th√¨ c√¢y Left c√≥ t·ªâ l·ªá 0.3 0.7; c√≤n c√¢y Right c√≥ t·ªâ l·ªá 0.8 0.2 

‚Üí c√¢y left c√≥: gini = 1 - 0.3^2 - 0.7^2 = 0.42

‚Üí c√¢y right c√≥: gini =. 1 - 0.2^2 - 0.8^2 = 0.32

T·ªïng gini = 0.42 + 0.32

Nh∆∞ v·∫≠y gini c√†ng b√© ch·ª©ng t·ªè c√†ng c√≥ kh·∫£ nƒÉng ph√¢n t√°ch t·ªët

Note: Ngo√†i gini c√≤n c√≥ th·ªÉ d√πng entropy, logloss ƒë·ªÉ chia c√¢y

![image.png](images/2.png)

C√¢u h·ªèi: C√≥ c·∫ßn thi·∫øt ph·∫£i duy·ªát qua h·∫øt t·∫•t c·∫£ feature l·∫°i kh√¥ng? V√† v·ªõi m·ªói feature c√≥ c·∫ßn duy·ªát h·∫øt c√°c ng∆∞·ª°ng ƒë·ªÉ c·∫Øt kh√¥ng?

tr·∫£ l·ªùi t·ª´ chatgpt:

- **Thu·∫≠t to√°n c∆° b·∫£n**:
    
    C√≥, v·ªõi feature d·∫°ng s·ªë (continuous), thu·∫≠t to√°n s·∫Ω x√©t t·∫•t c·∫£ c√°c gi√° tr·ªã c√≥ th·ªÉ l√†m ng∆∞·ª°ng chia. Th∆∞·ªùng l√† c√°c gi√° tr·ªã trung ƒëi·ªÉm gi·ªØa hai quan s√°t li√™n ti·∫øp sau khi s·∫Øp x·∫øp.
    
- **Chi ti·∫øt**:
    - N·∫øu c√≥ n gi√° tr·ªã kh√°c nhau cho feature ƒë√≥, s·∫Ω c√≥ t·ªëi ƒëa n‚àí1 ng∆∞·ª°ng c·∫ßn x√©t.
        
        n ‚Üí n-1
        
    - V·ªõi feature d·∫°ng ph√¢n lo·∫°i (categorical), thu·∫≠t to√°n s·∫Ω x√©t c√°c c√°ch nh√≥m c√°c category (nh∆∞ng th∆∞·ªùng d√πng heuristic v√¨ s·ªë t·ªï h·ª£p r·∫•t l·ªõn).
- **T·ªëi ∆∞u h√≥a**:
    - **Approximation**: Ch·ªâ x√©t m·ªôt s·ªë ng∆∞·ª°ng ƒë∆∞·ª£c ch·ªçn ng·∫´u nhi√™n ho·∫∑c d·ª±a tr√™n histogram.
    - **Pre-sorting**: S·∫Øp x·∫øp tr∆∞·ªõc ƒë·ªÉ gi·∫£m chi ph√≠ t√≠nh to√°n.

C√¢u h·ªèi: th·ª±c t·∫ø trong thu·∫≠t to√°n decision tree t·ª´ sklearn th√¨ h·ªç d√πng nh·ªØng kƒ© thu·∫≠t n√†o 

### **1. Kh√¥ng duy·ªát t·∫•t c·∫£ ng∆∞·ª°ng m·ªôt c√°ch brute-force**

- V·ªõi feature d·∫°ng s·ªë, scikit-learn **s·∫Øp x·∫øp tr∆∞·ªõc** c√°c gi√° tr·ªã c·ªßa feature v√† ch·ªâ x√©t c√°c ƒëi·ªÉm chia ·ªü **gi·ªØa hai gi√° tr·ªã li√™n ti·∫øp** (midpoints).

### **2. Pre-sorting v√† s·ª≠ d·ª•ng m·∫£ng ch·ªâ s·ªë**

- Thay v√¨ s·∫Øp x·∫øp l·∫°i d·ªØ li·ªáu ·ªü m·ªói node, scikit-learn **gi·ªØ m·ªôt m·∫£ng ch·ªâ s·ªë ƒë√£ s·∫Øp x·∫øp cho t·ª´ng feature**.
‚Üí Gi√∫p vi·ªác t√≠nh to√°n nhanh h∆°n khi ƒëi xu·ªëng c√°c node con.

### **3. Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng feature v√† ng∆∞·ª°ng**

- B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh:
    - `max_features`: Ch·ªâ x√©t m·ªôt t·∫≠p con ng·∫´u nhi√™n c·ªßa feature t·∫°i m·ªói node (gi·ªëng Random Forest).
    - `max_depth`, `min_samples_split`, `min_samples_leaf`: Gi·ªõi h·∫°n ƒë·ªô s√¢u v√† s·ªë l∆∞·ª£ng m·∫´u ƒë·ªÉ tr√°nh duy·ªát qu√° nhi·ªÅu.

### **4. S·ª≠ d·ª•ng Cython ƒë·ªÉ tƒÉng t·ªëc**

- Ph·∫ßn l√µi c·ªßa thu·∫≠t to√°n ƒë∆∞·ª£c vi·∫øt b·∫±ng **Cython**, gi√∫p t√≠nh to√°n impurity v√† t√¨m split nhanh h∆°n nhi·ªÅu so v·ªõi Python thu·∫ßn.

### **5. Kh√¥ng x√©t t·∫•t c·∫£ t·ªï h·ª£p cho feature ph√¢n lo·∫°i**

- V·ªõi feature d·∫°ng categorical (n·∫øu ƒë∆∞·ª£c m√£ h√≥a th√†nh s·ªë), scikit-learn **ch·ªâ x√©t chia theo gi√° tr·ªã ƒë∆°n l·∫ª**, kh√¥ng x√©t t·∫•t c·∫£ nh√≥m category (v√¨ s·ªë t·ªï h·ª£p qu√° l·ªõn).

### **6. Heuristic cho d·ªØ li·ªáu l·ªõn**

- V·ªõi `criterion="gini"` ho·∫∑c `"entropy"`, scikit-learn t√≠nh to√°n impurity theo c√¥ng th·ª©c t·ªëi ∆∞u h√≥a ƒë·ªÉ gi·∫£m s·ªë ph√©p t√≠nh.
- C√≥ th·ªÉ d√πng `splitter="random"` ƒë·ªÉ ch·ªçn ng∆∞·ª°ng ng·∫´u nhi√™n thay v√¨ duy·ªát h·∫øt (gi√∫p tƒÉng t·ªëc).